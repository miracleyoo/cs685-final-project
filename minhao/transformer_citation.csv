text,citationCount,influentialCitationCount
"This work introduces a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network, and can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps.",4114,527
"This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.",2267,416
"This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.",1421,183
"Following prior work on long-sequence transformers, the Longformer is evaluated on character-level language modeling and achieves state-of-the-art results on text8 and enwik8 and pretrain Longformer and finetune it on a variety of downstream tasks.",533,131
"This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.",493,72
"This work proposes the convolution-augmented transformer for speech recognition, named Conformer, which significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.",320,63
"The proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9–21 on various downstream tasks, and the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training.",148,39
"This paper pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language, and showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks.",185,50
"LayerDrop, a form of structured dropout, is explored, which has a regularization effect during training and allows for efficient pruning at inference time, and shows that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance.",187,31
"The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features.",138,27
"It is proved with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large and using a large learning rate makes the training unstable.",124,9
An end-to-end speech recognition model with Transformer encoders that can be used in a streaming speech recognition system and shows that the full attention version of the model beats the-state-of-the art accuracy on the LibriSpeech benchmarks.,127,16
"A novel Texture Transformer Network for Image Super-Resolution (TTSR), in which the LR and Ref images are formulated as queries and keys in a transformer, respectively, which achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.",116,9
"CTRL is released, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior, providing more explicit control over text generation.",356,70
"This paper introduces and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2, and achieves state-of-the-art performance and close to human quality.",240,42
"Comprehensive experiments on both aligned and non-aligned multimodal time-series show that the MulT model outperforms state-of-the-art methods by a large margin, and empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed cross modal attention mechanism in MulT.",172,43
"It is demonstrated that on the widely used Librispeech benchmark, the proposed transformer-based AM outperforms the best published hybrid result by 19% to 26% relative when the standard n-gram language model (LM) is used.",102,6
"A multi-modal transformer to jointly encode the different modalities in video, which allows each of them to attend to the others, and a novel framework to establish state-of-the-art results for video retrieval on three datasets.",80,24
"This paper investigates the mobile setting for NLP tasks to facilitate the deployment on the edge devices and designs Lite Transformer, which demonstrates consistent improvement over the transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling.",68,9
"The Progressive Dynamic Hurdles method is developed, which allows us to dynamically allocate more resources to more promising candidate models on the computationally expensive WMT 2014 English-German translation task, and demonstrates consistent improvement over the Transformer on four well-established language tasks.",231,30
It is claimed that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next.,185,29
"It is demonstrated that a Transformer with the modified relative attention mechanism can generate minute-long compositions with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies.",167,31
"This paper proposes Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion.",161,29
A new vision Transformer is presented that capably serves as a general-purpose backbone for computer vision and has the flexibility to model at various scales and has linear computational complexity with respect to image size.,394,124
"First, convolutional self-attention is proposed by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism, and LogSparse Transformer is proposed, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget.",167,27
"The Action Transformer model for recognizing and localizing human actions in video clips is introduced and it is shown that by using high-resolution, person-specific, class-agnostic queries, the model spontaneously learns to track individual people and to pick up on semantic context from the actions of others.",282,25
"An emergent sequence-to-sequence model called Transformer achieves state-of-the-art performance in neural machine translation and other natural language processing applications, including the surprising superiority of Transformer in 13/15 ASR benchmarks in comparison with RNN.",299,20
"An open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism in the Transformer model is introduced.",176,7
"Levenshtein Transformer is developed, a new partially autoregressive model devised for more flexible and amenable sequence generation and a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature.",144,29
A literature review of these visual transformer models by categorizing them in different tasks and analyzing the advantages and disadvantages of these methods is provided.,52,2
"This work proposes an end-to-end transformer model, which employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements.",219,42
The core idea of RoI Transformer is to apply spatial transformations on RoIs and learn the transformation parameters under the supervision of oriented bounding box (OBB) annotations.,124,33
"The Insertion Transformer outperforms many prior non-autoregressive approaches to translation at comparable or better levels of parallelism, and successfully recovers the performance of the original Transformer while requiring only logarithmically many iterations during decoding.",143,21
"This work proposes Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks without convolutions and could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches.",182,53
"It is found that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers, and the deepest layers of the model capture the most distant relationships.",102,7
"This paper adopts the stacked transformer architecture, but generalizes its training objective to maximize the mutual information between the masked signals, and the bidirectional context, via contrastive loss, which enables the model to handle continuous signals, such as visual features.",107,14
"This work generalizes a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood, and significantly increases the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks.",446,26
This work integrates connectionist temporal classification (CTC) with Transformer for joint training and decoding of automatic speech recognition (ASR) tasks and makes training faster than with RNNs and assists LM integration.,92,7
"The Speech-Transformer is presented, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency and a 2D-Attention mechanism which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech- Transformer.",287,26
"This work extends the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder, and introduces a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document- level parallel Corpora.",134,43
A Transformer-based sequence modeling framework built only with attention layers and feedforward layers that enables the Transformer to exploit semantic and visual information simultaneously and achieves state-of-the-art performance on the MSCOCO image captioning dataset.,77,11
"Competitive results using a Transformer encoder-decoder-attention model for end-to-end speech recognition needing less training time compared to a similarly performing LSTM model are presented and it is observed that the Transformer training is in general more stable compared to the L STM, although it also seems to overfit more, and thus shows more problems with generalization.",67,3
"The proposed Transformer-Transducer outperforms neural transducer with LSTM/BLSTM networks and achieved word error rates of 6.37 % on the test-clean set and 15.30%) while remaining streamable, compact, and computationally efficient with complexity of O(T), where T is input sequence length.",61,8
"TENER, a NER architecture adopting adapted Transformer Encoder to model the character- level features and word-level features is proposed, proving the Transformer-like encoder is just as effective for NER as other NLP tasks.",58,11
"A novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD) with tensor train decomposition is proposed, which can not only largely compress the model parameters but also obtain performance improvements.",57,3
"Star-Transformer replaces the fully-connected structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node, and complexity is reduced from quadratic to linear, while preserving the capacity to capture both local composition and long-range dependency.",55,3
"The site tests of the PV power plant with transformer integrated filtering method show that the PV grid-connected system comprised of the two-stage filtering station has the characteristics of low harmonic emission, high power factor, and stable operation.",62,1
"A new formulation of attention via the lens of the kernel is presented, which models the input as a product of symmetric kernels and achieves competitive performance to the current state of the art model with less computation.",44,7
"This work shows how to effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets, and achieves state-of-the-art results on multiple video classification benchmarks.",78,25
"A novel framework based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing, is presented, which is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning.",80,14
"It is pointed out that the attention inside these local patches are also essential for building visual transformers with high performance and a new architecture, namely, Transformer iN Transformer (TNT), is explored.",108,15
"To maximally excavate the capability of transformer, the IPT model is presented to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs and the contrastive learning is introduced for well adapting to different image processing tasks.",130,10
"The true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models is investigated and a model that learns synthetic attention weights without token-token interactions is proposed, called Synthesizer.",92,15
"A dual-branch transformer to combine image patches of different sizes to produce stronger image features and a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches.",62,6
"The experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model are described, confirming the general mantra “more data and larger models”.",168,10
"This work investigates the information that is learned by the attention mechanism in Transformer models with different translation quality, and sheds light on the relative strengths and weaknesses of the various encoder representations.",153,8
"PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations and is a network invariant to translation and equivariant to both rotation and scale, which is extensible to 3D which is demonstrated through the Cylindrical Trans transformer Network.",95,8
"It is demonstrated that a Transformer with the modified relative attention mechanism can generate minute-long compositions with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies.",92,8
"This work considers both the original Transformer Network (TF) and the larger Bidirectional Transformer (BERT), state-of-the-art on all natural language processing tasks, and proposes the novel use of Transformer Networks for trajectory forecasting.",45,8
The proposed average attention network is applied on the decoder part of the neural Transformer to replace the original target-side self-attention model and enables the neuralTransformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance.,88,13
"This work proposes SortNet, as part of the Point Transformer, which induces input permutation invariance by selecting points based on a learned score, to extract local and global features and relate both representations by introducing the local-global attention mechanism.",39,5
"This paper introduces and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2, and achieves state-of-the-art performance and close to human quality.",77,16
"This paper presents a high-frequency modular medium-voltage AC to low- voltage DC (400 VDC) system that is scalable in order to be used for different scale microgrids and focuses on high- frequencies transformer design to realize high-voltages insulation, high efficiency, and high density at the same time.",62,1
"This paper proposes a fast and efficient transformer differential protection scheme with additional differential CT saturation and cross-country fault detection modules after the external fault detection, all of them based on the differential wavelet coefficient energy with border distortions in order to stabilize the relay during external faults and distinguish accurately CT saturation from cross-Country internal faults.",60,1
A novel matrix transformer structure is proposed to integrate four elemental transformers into one magnetic core with simple four-layer print circuit board windings implementation and further reduced core loss by pushing switching frequency up to megahertz with GaN devices.,141,7
"It is demonstrated that IC-STNs can achieve better performance than conventional STNs with less model capacity, in particular, they show superior performance in pure image alignment tasks as well as joint alignment/classification problems on real-world problems.",111,18
"Weighted Transformer is proposed, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15-40% faster.",93,5
"Recently, the world's first ever power electronic traction transformer (PETT) for a 15-kV 16.7-Hz railway grid has been newly developed, commissioned, and installed on the test locomotive, where the PETT is presently in use.",256,16
This paper analyzes the steady-state operation and the range of zero-voltage switching in an ac–ac dual-active-bridge (DAB) converter for a solid-state transformer and develops a switch commutation scheme for the ac-ac DAB converters.,177,4
