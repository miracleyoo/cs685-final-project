{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import codecs\n",
    "import threading\n",
    "import urllib.parse as up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://www.semanticscholar.org/search?fos%5B0%5D=computer-science&q=transformer&sort=relevance&page=6'\n",
    "html = str(request.urlopen(url).read(), 'utf-8')\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdfetrgd'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'asdf'+'etr'+'gd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSSQuery(query, num_item=50, offset=0):\n",
    "    query = '+'.join(query.split())\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/search?query={query}&offset={offset}&limit={num_item}'\n",
    "    paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "    paper_list = json.loads(paper_list)\n",
    "\n",
    "    tldr_cat = ''\n",
    "    for paper in paper_list['data']:\n",
    "        paper_id = paper['paperId']\n",
    "        paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=citations.authors,tldr,'\n",
    "        paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "        paper_info = json.loads(paper_info)\n",
    "        tldr = paper_info['tldr']\n",
    "        if tldr is not None:\n",
    "            tldr_cat += ' '+tldr['text']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 12364998, 'offset': 0, 'next': 30, 'data': [{'paperId': '32186d6d0e11039bd4c0aca43b3c7858b2fe0ed0', 'title': 'Calibration of Instrument Current Transformer Test Sets'}, {'paperId': 'c0ab29fec845d7e7bcedf50cc05bad440dfd02cd', 'title': 'A Computer-Controlled Calibrator for Instrument Transformer Test Sets'}, {'paperId': '2c38ce741b2d14510348450225a6cf27fd11d79a', 'title': 'Analysis of Quality Problems in Power Transformer Test Check and Their Control Measure Research'}, {'paperId': '0170fc76e934ee643f869df18fb617d5357e8b4e', 'title': 'Conformer: Convolution-augmented Transformer for Speech Recognition'}, {'paperId': 'f481daf0a28292ac36e94611e538c1a3b3ad71c5', 'title': 'Non-conventional instrument current transformer test set for industrial applications'}, {'paperId': '8d6f2fb79644f0664b6c2944e00a358802ac18cb', 'title': 'Instrument Transformer Test Set calibration using digital sampling'}, {'paperId': 'f2e655902bc78fe3f796cb40b03f7fe030d79beb', 'title': 'A current transformer test set for the audio frequency range'}, {'paperId': 'ba8bebb305fb0c472a7e9e3088607d88220d4370', 'title': 'Transformer Test System'}, {'paperId': '950c0c1d21a969d3498cb8ee11607e53127e6220', 'title': 'Transformer test tool'}, {'paperId': '3c5882ba83265093f2625fcebaf41bcdae4548a1', 'title': 'Meshed-Memory Transformer for Image Captioning'}, {'paperId': '6cbf8ee908cb572a37c91ed2de401421378b7b88', 'title': 'A complex current ratio device for the calibration of current transformer test sets.'}, {'paperId': '0e75e6b61f9a7d15f8c21b3114665e7510cb1399', 'title': 'Application of Digital Sampling Method for Voltage Transformer Test Set Calibrations'}, {'paperId': '9f2dd5cc190fc713f1339fca838a5537931744f8', 'title': 'Neural Speech Synthesis with Transformer Network'}, {'paperId': '45c0e9d42624d02752b2c5adce6032db1deb2940', 'title': 'Streaming Automatic Speech Recognition with the Transformer Model'}, {'paperId': '94bbc4ea271c918705876b60d98d227a0ab55a43', 'title': 'Video Action Transformer Network'}, {'paperId': 'fd9649aa3b3151615fffb6bd1b547ee7d82766ee', 'title': 'Transformer-Transducer: End-to-End Speech Recognition with Self-Attention'}, {'paperId': '62dc8ddb4907db4b889c5e93673d9b3c189d1f25', 'title': 'A Tensorized Transformer for Language Modeling'}, {'paperId': 'eb421e3d482fc169f08a0aad2f4c605c1c62b944', 'title': 'A Transformer Model for Retrosynthesis'}, {'paperId': 'ba24b67fc98bb3f65e616bfeda941069f185d50f', 'title': 'Analysis and Test Results of a Brushless Doubly Fed Induction Machine With Rotary Transformer'}, {'paperId': '94238dead40b12735d79ed63e29ead70730261a2', 'title': 'An Analysis of Encoder Representations in Transformer-Based Machine Translation'}, {'paperId': '492e4521022c25b99e61e72d1eb6fc4fa112c496', 'title': 'Study on Oscillating Switching Impulse Voltage Generation for Power Transformer Onsite Test'}, {'paperId': '7219ca6da41f1084d9bb4541a7d576da03428f87', 'title': 'Partial discharge criterion in AC test of oil-immersed transformer and gas-filled transformer in terms of harmful partial discharge level and signal transmission rate'}, {'paperId': 'cda16850d5b473a6ddf6b59b6294b2bb46155518', 'title': 'Quality Test and Problem Analysis of Electronic Transformer'}, {'paperId': '21e0c74f624c27334e62a3dcdc4eb2c4a5316d70', 'title': 'Close to Human Quality TTS with Transformer'}, {'paperId': 'a803a16fcbc11bbcf1fef75a70018bff41f703b5', 'title': 'Causes of transformer failures and diagnostic methods – A review'}, {'paperId': '3ef4376ded6f8a1bb8b14b6b45be2741e49f17c9', 'title': 'Experimental thermodynamic evaluation for a single stage heat transformer prototype build with commercial PHEs'}, {'paperId': '177984285afa5474b96fc5d97c5723ee5debdebd', 'title': 'Development and simulation evaluation of a magnetorheological elastomer isolator for transformer vibration control'}, {'paperId': 'c5d5655935a85ba9ef68449925752c14de7af32e', 'title': 'Verification of a Low Component Nine-Level Cascaded-Transformer Multilevel Inverter in Grid-Tied Mode'}, {'paperId': '1419ac21278858ec2b5824efdeb769792923efda', 'title': 'Concurrent Voltage Control and Dispatch of Active Distribution Networks by Means of Smart Transformer and Storage'}, {'paperId': 'e8c4a75ecc90a314e67cbc037de4e63b106146ff', 'title': 'HTS Transformer: Construction Details, Test Results, and Noted Failure Mechanisms'}]}\n"
     ]
    }
   ],
   "source": [
    "num_item = 30\n",
    "offset = 0\n",
    "query = 'transformer test'\n",
    "query = '+'.join(query.split())\n",
    "url = f'https://api.semanticscholar.org/graph/v1/paper/search?query={query}&offset={offset}&limit={num_item}'\n",
    "paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "paper_list = json.loads(paper_list)\n",
    "print(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper presents the basic layout for instrument current transformer calibration using comparative method with a standard and results obtained using described procedure are presented.\n",
      "A new calibrator is described that reaches uncertainties of the order of $10\\times 10^{-6}$ while being suitable for the calibration of test sets for nonconventional analog instrument transformers and fully computer-controlled.\n",
      "This work proposes the convolution-augmented transformer for speech recognition, named Conformer, which significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.\n",
      "A test set for the calibration of non-conventional current instrument transformer working with current down to the 0.1 mA level, intended for use in industrial environment and with target uncertainties within 100 μA/A and 100 μrad.\n",
      "The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features.\n",
      "This paper introduces and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2, and achieves state-of-the-art performance and close to human quality.\n",
      "This work proposes a transformer based end-to-end ASR system for streaming ASR, where an output must be generated shortly after each spoken word, and applies time-restricted self-attention for the encoder and triggered attention for theEncoder-decoder attention mechanism.\n",
      "The Action Transformer model for recognizing and localizing human actions in video clips is introduced and it is shown that by using high-resolution, person-specific, class-agnostic queries, the model spontaneously learns to track individual people and to pick up on semantic context from the actions of others.\n",
      "The proposed Transformer-Transducer outperforms neural transducer with LSTM/BLSTM networks and achieved word error rates of 6.37 % on the test-clean set and 15.30%) while remaining streamable, compact, and computationally efficient with complexity of O(T), where T is input sequence length.\n",
      "A novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD) with tensor train decomposition is proposed, which can not only largely compress the model parameters but also obtain performance improvements.\n",
      "A Transformer model for a retrosynthetic reaction prediction task is described and it is found that snapshot learning with averaging weights on learning rates minima works best.\n",
      "This paper analyzes a 90-kW brushless doubly fed three-phase induction machine in which a wound rotor circuit is connected to a rotary transformer and presents the advantages of substituting brushes and slip rings by aRotary transformer.\n",
      "This work investigates the information that is learned by the attention mechanism in Transformer models with different translation quality, and sheds light on the relative strengths and weaknesses of the various encoder representations.\n",
      "An oscillating switching impulse voltage generation method using a power transformer induction principle through which the power transformer itself can produce the oscillating switch impulse voltage in accordance with the requirements of the IEC60060-3 standard is described.\n",
      "This paper introduces and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2, and achieves state-of-the-art performance and close to human quality.\n",
      "A simple and compact structure for transformer-based multilevel inverters is introduced since the number of utilized components in the proposed structure is remarkably reduced, the cost, volume, and complexity are minimized.\n",
      "This work proposes a control strategy where the storage system is used to achieve dispatched-by-design operation of the LV network active power flow, as well as the two ST power converters to control the voltage in both the MV and LV grid on a best effort basis.\n"
     ]
    }
   ],
   "source": [
    "for paper in paper_list['data']:\n",
    "    paper_id = paper['paperId']\n",
    "    paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=citations.authors,tldr'\n",
    "    paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "    paper_info = json.loads(paper_info)\n",
    "    try:\n",
    "        print(paper_info['tldr']['text'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(*args, end=None):\n",
    "    if end is None:\n",
    "        print(time.strftime(\"==> [%Y-%m-%d %H:%M:%S]\",\n",
    "                            time.localtime()) + \" \" + \"\".join([str(s) for s in args]))\n",
    "    else:\n",
    "        print(time.strftime(\"==> [%Y-%m-%d %H:%M:%S]\", time.localtime()) + \" \" + \"\".join([str(s) for s in args]),\n",
    "              end=end)\n",
    "\n",
    "def SSSCrawl(urls, outpath):\n",
    "    for url in urls:\n",
    "        tldr_name = up.unquote(url).split('/')[-1].replace(' ', '-')[15:]\n",
    "        tldr_path = os.path.join(outpath, 'txt', tldr_name)\n",
    "        try:\n",
    "            tldr = requests.get(url)\n",
    "            with open(tldr_path, 'wb') as f:\n",
    "                f.write(tldr.content)\n",
    "                f.flush()\n",
    "            log(\"Successfully crawled tldr:{}\".format(tldr_name[:60]))\n",
    "        except:\n",
    "            log(\"Failed crawling tldr:{}\".format(url))\n",
    "            \n",
    "def SSSCrawlMain(url, outpath):\n",
    "    if not os.path.exists(outpath):\n",
    "        os.mkdir(outpath)\n",
    "    if not os.path.exists(os.path.join(outpath, 'txt')):\n",
    "        os.mkdir(os.path.join(outpath, 'txt'))\n",
    "    html = str(request.urlopen(url).read(), 'utf-8')\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    try:\n",
    "        pagesum = int(soup.body.select(\n",
    "            '#content #post-list .content #paginator .pagination a')[-2].get_text())\n",
    "    except IndexError:\n",
    "        pagesum = 1\n",
    "\n",
    "    log('Start crawling tldr links...')\n",
    "    links = []\n",
    "    for pagenum in range(1, pagesum+1):\n",
    "        if page>1:\n",
    "            _url = url + '&page=' + str(pagenum)\n",
    "        html = str(request.urlopen(_url).read(), 'utf-8')\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        links.extend([i.attrs['href'] for i in soup.body.select(\n",
    "            '#content > #post-list > .content > div > #post-list-posts > li > a')])\n",
    "    log('Finished crawling image links.')\n",
    "\n",
    "    with codecs.open(os.path.join(outpath, 'url_list.txt'), 'w+') as f:\n",
    "        for link in links:\n",
    "            f.write(link+'\\n')\n",
    "\n",
    "    def chunks(arr, m):\n",
    "        n = int(math.floor(len(arr) / float(m)))\n",
    "        return [*[arr[i:i + n] for i in range(0, (m-1)*n, n)],arr[(m-1)*n:]]\n",
    "\n",
    "    threads = []\n",
    "    links_sep = chunks(links, opt.thread_num)\n",
    "\n",
    "    for i in range(opt.thread_num):\n",
    "        threads.append(imgThread(i, links_sep[i], outpath))\n",
    "\n",
    "    for i in threads:\n",
    "        i.start()\n",
    "\n",
    "    for i in threads:\n",
    "        i.join()\n",
    "    log('Successfully crawled all of the images. Thanks for using.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.6.4-cp36-cp36m-win_amd64.whl (3.5 MB)\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-4.6.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--key', default='Visible Light Communication', help='The name of tag you want to crawl.')\n",
    "    parser.add_argument('--thread_num', type=int, default=5, help='The number of threads you want to use when crawling.')\n",
    "    opt = parser.parse_args()\n",
    "    KonaTagCrawlMain('https://www.semanticscholar.org/search?q='+opt.key+'sort=relevance', opt.key)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ae9f0abca11b5964ed3b2e1eeca7b20601143dc7930a532e9cd7a2d36507b9d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('carla': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
