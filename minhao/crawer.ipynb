{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "\n",
    "import codecs\n",
    "import urllib.parse as up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler and Semantic Scholar Information Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a concatenated tldr string of papers from a certain query. Also return the information pack of these papers.\n",
    "def SSSQuery(query, num_item=50, offset=0, fos=None):\n",
    "    # Send a Semantic API post to get the result paper list\n",
    "    query = '+'.join(query.split())\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/search?query={query}&offset={offset}&limit={num_item}&fields=fieldsOfStudy,abstract'\n",
    "    paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "    paper_list = json.loads(paper_list)\n",
    "\n",
    "    tldr_cat = ''\n",
    "    info_pack = []\n",
    "    # Loop through the paper list and get information pack for each paper\n",
    "    for paper in paper_list['data']:\n",
    "        paper_id = paper['paperId']\n",
    "        # Each paper info pack consists title, abstract, authors, tldr, citation count, field of study\n",
    "        paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=title,abstract,citations.authors,tldr,citationCount,fieldsOfStudy'\n",
    "        paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "        paper_info = json.loads(paper_info)\n",
    "        # Filter out the papers out of field of study\n",
    "        if fos is not None and fos not in paper_info['fieldsOfStudy']:\n",
    "            continue\n",
    "        info_pack.append(paper_info)\n",
    "        tldr = paper_info['tldr']\n",
    "        if tldr is not None:\n",
    "            tldr_cat += ' '+tldr['text'].strip()\n",
    "    tldr_cat = tldr_cat.strip()\n",
    "    return tldr_cat, info_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spatial Transformer Networks', 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context', 'Longformer: The Long-Document Transformer', 'Reformer: The Efficient Transformer', 'Conformer: Convolution-augmented Transformer for Speech Recognition', 'Heterogeneous Graph Transformer', 'AraBERT: Transformer-based Model for Arabic Language Understanding', 'Reducing Transformer Depth on Demand with Structured Dropout', 'Meshed-Memory Transformer for Image Captioning']\n"
     ]
    }
   ],
   "source": [
    "tldr_cat, info_pack = SSSQuery('transformer', num_item=10, fos='Computer Science')\n",
    "print([i['title'] for i in info_pack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work introduces a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network, and can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps. This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Following prior work on long-sequence transformers, the Longformer is evaluated on character-level language modeling and achieves state-of-the-art results on text8 and enwik8 and pretrain Longformer and finetune it on a variety of downstream tasks. This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences. This work proposes the convolution-augmented transformer for speech recognition, named Conformer, which significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. The proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9–21 on various downstream tasks, and the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. This paper pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language, and showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. LayerDrop, a form of structured dropout, is explored, which has a regularization effect during training and allows for efficient pruning at inference time, and shows that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tldr_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spatial', 0.03213113441634914)\n",
      "('work', 0.04798022059872212)\n",
      "('language', 0.06973861700167353)\n",
      "('Transformer', 0.07994217878795795)\n",
      "('tasks', 0.08425923835568348)\n",
      "('architectures', 0.12034394262796394)\n",
      "('Longformer', 0.1231255301810972)\n",
      "('neural', 0.13795581464104506)\n",
      "('training', 0.13840506098154778)\n",
      "('module', 0.14654904914113015)\n",
      "('giving', 0.14654904914113015)\n",
      "('maps', 0.14654904914113015)\n",
      "('achieves', 0.1514314025909243)\n",
      "('results', 0.1514314025909243)\n",
      "('introduces', 0.15751851687783422)\n",
      "('learnable', 0.15751851687783422)\n",
      "('explicitly', 0.15751851687783422)\n",
      "('manipulation', 0.15751851687783422)\n",
      "('data', 0.15751851687783422)\n",
      "('inserted', 0.15751851687783422)\n"
     ]
    }
   ],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 20\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(tldr_cat)\n",
    "keywords = sorted(keywords, key=lambda x:x[1])\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.4.0\n",
      "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (0.8)\n",
      "Requirement already satisfied: packaging in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (21.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (0.1.96)\n",
      "Requirement already satisfied: sacremoses in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (0.0.46)\n",
      "Requirement already satisfied: filelock in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (3.4.0)\n",
      "Requirement already satisfied: requests in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (2.24.0)\n",
      "Requirement already satisfied: protobuf in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (3.13.0)\n",
      "Requirement already satisfied: numpy in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (4.62.3)\n",
      "Collecting tokenizers==0.9.2\n",
      "  Downloading tokenizers-0.9.2-cp36-cp36m-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from transformers==3.4.0) (2021.11.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: joblib in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from sacremoses->transformers==3.4.0) (1.1.0)\n",
      "Requirement already satisfied: six in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from sacremoses->transformers==3.4.0) (1.15.0)\n",
      "Requirement already satisfied: click in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from requests->transformers==3.4.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from requests->transformers==3.4.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from requests->transformers==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from requests->transformers==3.4.0) (1.25.10)\n",
      "Requirement already satisfied: setuptools in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from protobuf->transformers==3.4.0) (49.6.0.post20200814)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from tqdm>=4.27->transformers==3.4.0) (0.4.3)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.12.5\n",
      "    Uninstalling transformers-4.12.5:\n",
      "      Successfully uninstalled transformers-4.12.5\n",
      "Successfully installed tokenizers-0.9.2 transformers-3.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "sentence-transformers 2.1.0 requires tokenizers>=0.10.3, but you'll have tokenizers 0.9.2 which is incompatible.\n",
      "sentence-transformers 2.1.0 requires transformers<5.0.0,>=4.6.0, but you'll have transformers 3.4.0 which is incompatible.\n",
      "flair 0.7 requires sentencepiece<=0.1.91, but you'll have sentencepiece 0.1.96 which is incompatible.\n",
      "flair 0.7 requires transformers<=3.5.1,>=3.5.0, but you'll have transformers 3.4.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==3.1.0a0\n",
      "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
      "Collecting httpx==0.13.3\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Collecting httpcore==0.9.*\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Collecting hstspreload\n",
      "  Downloading hstspreload-2021.11.1-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: certifi in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.6.20)\n",
      "Requirement already satisfied: chardet==3.* in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
      "Collecting rfc3986<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Collecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting contextvars>=2.1; python_version < \"3.7\"\n",
      "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting immutables>=0.9\n",
      "  Downloading immutables-0.16-cp36-cp36m-win_amd64.whl (59 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3; python_version < \"3.8\" in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from immutables>=0.9->contextvars>=2.1; python_version < \"3.7\"->sniffio->httpx==0.13.3->googletrans==3.1.0a0) (3.7.4.3)\n",
      "Building wheels for collected packages: googletrans, contextvars\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16378 sha256=8f4674fbb2ff2330cfd0f3f712c9fbc3c1a2a456af2b81b32691b457dcf47c07\n",
      "  Stored in directory: c:\\users\\minha\\appdata\\local\\pip\\cache\\wheels\\26\\36\\33\\1ead496ea4484c4df14c9d148c9e2676a1a2d821bc88d4f453\n",
      "  Building wheel for contextvars (setup.py): started\n",
      "  Building wheel for contextvars (setup.py): finished with status 'done'\n",
      "  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7669 sha256=b2a2d93abe5e6689dbaf5986aade223becdd5f970a8084efc26af3e07f21c5e2\n",
      "  Stored in directory: c:\\users\\minha\\appdata\\local\\pip\\cache\\wheels\\41\\11\\53\\911724983aa48deb94792432e14e518447212dd6c5477d49d3\n",
      "Successfully built googletrans contextvars\n",
      "Installing collected packages: hpack, hyperframe, h2, h11, immutables, contextvars, sniffio, httpcore, hstspreload, rfc3986, httpx, googletrans\n",
      "Successfully installed contextvars-2.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.11.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 immutables-0.16 rfc3986-1.5.0 sniffio-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting celery==4.4.2\n",
      "  Downloading celery-4.4.2-py2.py3-none-any.whl (422 kB)\n",
      "Collecting billiard<4.0,>=3.6.3.0\n",
      "  Downloading billiard-3.6.4.0-py3-none-any.whl (89 kB)\n",
      "Collecting pytz>dev\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting vine==1.3.0\n",
      "  Downloading vine-1.3.0-py2.py3-none-any.whl (14 kB)\n",
      "Collecting kombu<4.7,>=4.6.8\n",
      "  Downloading kombu-4.6.11-py2.py3-none-any.whl (184 kB)\n",
      "Collecting amqp<2.7,>=2.6.0\n",
      "  Downloading amqp-2.6.1-py2.py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.18; python_version < \"3.8\" in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from kombu<4.7,>=4.6.8->celery==4.4.2) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from importlib-metadata>=0.18; python_version < \"3.8\"->kombu<4.7,>=4.6.8->celery==4.4.2) (3.1.0)\n",
      "Installing collected packages: billiard, pytz, vine, amqp, kombu, celery\n",
      "Successfully installed amqp-2.6.1 billiard-3.6.4.0 celery-4.4.2 kombu-4.6.11 pytz-2021.3 vine-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install celery==4.4.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy>=1.14.0 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from seqeval) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from seqeval) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\users\\minha\\anaconda3\\envs\\cs682\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16175 sha256=fdc9b5b4982ca8a2e5f11506fd1729be7df2837efc779ee1cbc9a4b236a857dc\n",
      "  Stored in directory: c:\\users\\minha\\appdata\\local\\pip\\cache\\wheels\\39\\29\\36\\1c4f7905c133e11748ca375960154964082d4fb03478323089\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conllu\n",
      "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: conllu\n",
      "Successfully installed conllu-4.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03500243 0.04180846 0.01458435 0.0405931  0.01871658 0.01871658\n",
      " 0.01895965 0.07778318 0.02479339 0.02236266 0.0405931  0.02600875\n",
      " 0.01944579 0.03475936 0.01093826 0.02236266 0.0449684  0.05833738\n",
      " 0.0140982  0.01652893 0.55104521 0.03014098 0.03014098 0.03597472\n",
      " 0.08653379 0.07267866 0.01944579 0.02309188 0.04302382 0.04083617\n",
      " 0.04423918 0.01263977 0.02698104 0.03719008 0.033544   1.\n",
      " 0.00947982 0.02625182 0.03427321 0.09577054 0.01336898 0.02479339\n",
      " 0.02236266 0.01628585 0.0308702  0.06976179 0.06854643 0.04545455\n",
      " 0.01507049 0.03257171 0.02139037 0.0281964  0.11983471 0.04278075\n",
      " 0.03913466 0.01482742 0.05614973 0.05323286 0.34540593 0.01507049\n",
      " 0.03159942 0.06222654 0.10841031 0.0449684  0.01385513 0.02260574\n",
      " 0.01507049 0.12955761 0.01069519]\n",
      "Original:  Levenshtein Transformer is developed, a new partially autoregressive model devised for more flexible and amenable sequence generation and a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature.\n",
      "Token IDs: tensor([  101, 23310,  6132, 11039, 12377, 10938,  2121,  2003,  2764,  1010,\n",
      "         1037,  2047,  6822,  8285,  2890, 17603, 18719,  3726,  2944, 14917,\n",
      "         2005,  2062, 12379,  1998,  2572,  8189,  3468,  5537,  4245,  1998,\n",
      "         1037,  2275,  1997,  2047,  2731,  5461,  4056,  2012,  2068,  1010,\n",
      "         6464, 18077,  2075,  2028,  2004,  1996,  2060,  1005,  1055,  4083,\n",
      "         4742,  4283,  2000,  2037, 21053,  3267,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "from helpers import tokenize_and_format, flat_accuracy\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('transformer_citation.csv')\n",
    "# df = pd.read_csv('tweets.csv')\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "texts = df.text.values\n",
    "labels = df.citationCount.values\n",
    "\n",
    "labels =labels/labels.max()\n",
    "\n",
    "# print(labels)\n",
    "### tokenize_and_format() is a helper function provided in helpers.py ###\n",
    "input_ids, attention_masks = tokenize_and_format(texts)\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.FloatTensor(labels)\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', texts[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total = len(df)\n",
    "\n",
    "num_train = int(total * .6)\n",
    "num_val = int(total * .2)\n",
    "num_test = total - num_train - num_val\n",
    "\n",
    "# make lists of 3-tuples (already shuffled the dataframe in cell above)\n",
    "\n",
    "train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]\n",
    "val_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train, num_val+num_train)]\n",
    "test_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_val + num_train, total)]\n",
    "\n",
    "train_text = [texts[i] for i in range(num_train)]\n",
    "val_text = [texts[i] for i in range(num_train, num_val+num_train)]\n",
    "test_text = [texts[i] for i in range(num_val + num_train, total)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 1, # The number of output labels.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8\n",
    "                  weight_decay = 0.01\n",
    "                )\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# function to get validation accuracy\n",
    "def get_validation_performance(val_set):\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    num_batches = int(len(val_set)/batch_size) + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "      end_index = min(batch_size * (i+1), len(val_set))\n",
    "\n",
    "      batch = val_set[i*batch_size:end_index]\n",
    "\n",
    "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
    "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
    "      label_tensors = torch.stack([data[2] for data in batch])\n",
    "      \n",
    "      # Move tensors to the GPU\n",
    "      b_input_ids = input_id_tensors.to(device)\n",
    "      b_input_mask = input_mask_tensors.to(device)\n",
    "      b_labels = label_tensors.to(device)\n",
    "        \n",
    "      # Tell pytorch not to bother with constructing the compute graph during\n",
    "      # the forward pass, since this is only needed for backprop (training).\n",
    "      with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "        loss = outputs[0]#.loss\n",
    "        logits = outputs[1]#.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().numpy()\n",
    "        label_ids = b_labels.numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches. We provide a function in \n",
    "        # helpers.py called flat_accuracy() that takes in predictions and labels\n",
    "        # acc = flat_accuracy(logits, label_ids)       \n",
    "        # total_eval_accuracy += acc\n",
    "        error = np.abs(logits-label_ids).sum()\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_error = error / num_batches\n",
    "    return avg_error \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.019743618555366993\n",
      "Validation accuracy: 5.407197952270508\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.00995240593329072\n",
      "Validation accuracy: 9.79728889465332\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.01938678789883852\n",
      "Validation accuracy: 13.865350723266602\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.01795857585966587\n",
      "Validation accuracy: 9.393569946289062\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.01085454411804676\n",
      "Validation accuracy: 5.033558368682861\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.01806918578222394\n",
      "Validation accuracy: 5.784292221069336\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.018869482446461916\n",
      "Validation accuracy: 4.773448944091797\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.01139177568256855\n",
      "Validation accuracy: 5.468070030212402\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.01669137179851532\n",
      "Validation accuracy: 6.432164192199707\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "Total loss: 0.008767289575189352\n",
      "Validation accuracy: 6.560471057891846\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# training loop\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    # num_batches = int(len(train_set)/batch_size) + 1\n",
    "    num_batches = int(len(train_set)/batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "      end_index = min(batch_size * (i+1), len(train_set))\n",
    "\n",
    "      batch = train_set[i*batch_size:end_index]\n",
    "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
    "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
    "      label_tensors = torch.stack([data[2] for data in batch])\n",
    "\n",
    "      # Move tensors to the GPU\n",
    "      b_input_ids = input_id_tensors.to(device)\n",
    "      b_input_mask = input_mask_tensors.to(device)\n",
    "      b_labels = label_tensors.to(device)\n",
    "\n",
    "      # Clear the previously calculated gradient\n",
    "      model.zero_grad()        \n",
    "\n",
    "      # Perform a forward pass (evaluate the model on this training batch).\n",
    "      outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels=b_labels)\n",
    "      # print(outputs)\n",
    "      loss = outputs[0]#.loss\n",
    "      logits = outputs[1]#.logits\n",
    "\n",
    "      total_train_loss += loss.item()\n",
    "\n",
    "      # Perform a backward pass to calculate the gradients.\n",
    "      loss.backward()\n",
    "\n",
    "      # Update parameters and take a step using the computed gradient.\n",
    "      optimizer.step()\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set. Implement this function in the cell above.\n",
    "    print(f\"Total loss: {total_train_loss}\")\n",
    "    val_error = get_validation_performance(val_set)\n",
    "    print(f\"Validation accuracy: {val_error}\")\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neural architecture transformer', 0.5527), ('length disrupting', 0.1076), ('benchmarks covering summarization', 0.2611), ('language showed newly', 0.2474), ('explicitly allows spatial', 0.2433), ('replaces dot product', 0.2065), ('answering', 0.2506), ('finetune variety downstream', 0.0994), ('sampling algorithm hgsampling', -0.0439), ('nlp tasks layerdrop', 0.323)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# doc = \"\"\"\n",
    "#          Supervised learning is the machine learning task of learning a function that\n",
    "#          maps an input to an output based on example input-output pairs. It infers a\n",
    "#          function from labeled training data consisting of a set of training examples.\n",
    "#          In supervised learning, each example is a pair consisting of an input object\n",
    "#          (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "#          A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "#          which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "#          algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "#          the learning algorithm to generalize from the training data to unseen situations in a \n",
    "#          'reasonable' way (see inductive bias).\n",
    "#       \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(tldr_cat, keyphrase_ngram_range=(1,3), stop_words='english', top_n = 10, use_maxsum=True, nr_candidates=40, use_mmr=True, diversity=0.7)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_item = 30\n",
    "offset = 0\n",
    "query = 'hand'\n",
    "query = '+'.join(query.split())\n",
    "url = f'https://api.semanticscholar.org/graph/v1/paper/search?fos[0]=computer-science&query={query}&offset={offset}&limit={num_item}'\n",
    "paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "paper_list = json.loads(paper_list)\n",
    "# print(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in paper_list['data']:\n",
    "    paper_id = paper['paperId']\n",
    "    paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=citations.authors,tldr,citationCount,fieldsOfStudy'\n",
    "    paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "    paper_info = json.loads(paper_info)\n",
    "    print(paper_info['citationCount'], paper_info['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ae9f0abca11b5964ed3b2e1eeca7b20601143dc7930a532e9cd7a2d36507b9d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('carla': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
