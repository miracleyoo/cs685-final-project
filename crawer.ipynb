{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import yake\n",
    "\n",
    "import codecs\n",
    "import urllib.parse as up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler and Semantic Scholar Information Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a concatenated tldr string of papers from a certain query. Also return the information pack of these papers.\n",
    "def SSSQuery(query, num_item=50, offset=0, fos=None):\n",
    "    # Send a Semantic API post to get the result paper list\n",
    "    query = '+'.join(query.split())\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/search?query={query}&offset={offset}&limit={num_item}&fields=fieldsOfStudy,abstract'\n",
    "    paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "    paper_list = json.loads(paper_list)\n",
    "\n",
    "    tldr_cat = ''\n",
    "    info_pack = []\n",
    "    # Loop through the paper list and get information pack for each paper\n",
    "    for paper in paper_list['data']:\n",
    "        paper_id = paper['paperId']\n",
    "        # Each paper info pack consists title, abstract, authors, tldr, citation count, field of study\n",
    "        paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=title,abstract,citations.authors,tldr,citationCount,fieldsOfStudy'\n",
    "        paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "        paper_info = json.loads(paper_info)\n",
    "        # Filter out the papers out of field of study\n",
    "        if fos is not None and fos not in paper_info['fieldsOfStudy']:\n",
    "            continue\n",
    "        info_pack.append(paper_info)\n",
    "        tldr = paper_info['tldr']\n",
    "        if tldr is not None:\n",
    "            tldr_cat += ' '+tldr['text'].strip()\n",
    "    tldr_cat = tldr_cat.strip()\n",
    "    return tldr_cat, info_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4101, 2250, 1416, 531, 491, 313, 146, 184, 184, 134]\n"
     ]
    }
   ],
   "source": [
    "tldr_cat, info_pack = SSSQuery('transformer', num_item=10, fos='Computer Science')\n",
    "print([i['citationCount'] for i in info_pack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work introduces a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network, and can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps. This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Following prior work on long-sequence transformers, the Longformer is evaluated on character-level language modeling and achieves state-of-the-art results on text8 and enwik8 and pretrain Longformer and finetune it on a variety of downstream tasks. This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences. This work proposes the convolution-augmented transformer for speech recognition, named Conformer, which significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. The proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9–21 on various downstream tasks, and the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. This paper pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language, and showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. LayerDrop, a form of structured dropout, is explored, which has a regularization effect during training and allows for efficient pruning at inference time, and shows that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tldr_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spatial', 0.03213113441634914)\n",
      "('work', 0.04798022059872212)\n",
      "('language', 0.06973861700167353)\n",
      "('Transformer', 0.07994217878795795)\n",
      "('tasks', 0.08425923835568348)\n",
      "('architectures', 0.12034394262796394)\n",
      "('Longformer', 0.1231255301810972)\n",
      "('neural', 0.13795581464104506)\n",
      "('training', 0.13840506098154778)\n",
      "('module', 0.14654904914113015)\n",
      "('giving', 0.14654904914113015)\n",
      "('maps', 0.14654904914113015)\n",
      "('achieves', 0.1514314025909243)\n",
      "('results', 0.1514314025909243)\n",
      "('introduces', 0.15751851687783422)\n",
      "('learnable', 0.15751851687783422)\n",
      "('explicitly', 0.15751851687783422)\n",
      "('manipulation', 0.15751851687783422)\n",
      "('data', 0.15751851687783422)\n",
      "('inserted', 0.15751851687783422)\n"
     ]
    }
   ],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 20\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(tldr_cat)\n",
    "keywords = sorted(keywords, key=lambda x:x[1])\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neural architecture transformer', 0.5527), ('length disrupting', 0.1076), ('benchmarks covering summarization', 0.2611), ('language showed newly', 0.2474), ('explicitly allows spatial', 0.2433), ('replaces dot product', 0.2065), ('answering', 0.2506), ('finetune variety downstream', 0.0994), ('sampling algorithm hgsampling', -0.0439), ('nlp tasks layerdrop', 0.323)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# doc = \"\"\"\n",
    "#          Supervised learning is the machine learning task of learning a function that\n",
    "#          maps an input to an output based on example input-output pairs. It infers a\n",
    "#          function from labeled training data consisting of a set of training examples.\n",
    "#          In supervised learning, each example is a pair consisting of an input object\n",
    "#          (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "#          A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "#          which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "#          algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "#          the learning algorithm to generalize from the training data to unseen situations in a \n",
    "#          'reasonable' way (see inductive bias).\n",
    "#       \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(tldr_cat, keyphrase_ngram_range=(1,3), stop_words='english', top_n = 10, use_maxsum=True, nr_candidates=40, use_mmr=True, diversity=0.7)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_item = 30\n",
    "offset = 0\n",
    "query = 'hand'\n",
    "query = '+'.join(query.split())\n",
    "url = f'https://api.semanticscholar.org/graph/v1/paper/search?fos[0]=computer-science&query={query}&offset={offset}&limit={num_item}'\n",
    "paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "paper_list = json.loads(paper_list)\n",
    "# print(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in paper_list['data']:\n",
    "    paper_id = paper['paperId']\n",
    "    paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=citations.authors,tldr,citationCount,fieldsOfStudy'\n",
    "    paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "    paper_info = json.loads(paper_info)\n",
    "    print(paper_info['citationCount'], paper_info['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ae9f0abca11b5964ed3b2e1eeca7b20601143dc7930a532e9cd7a2d36507b9d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('carla': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
