{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import codecs\n",
    "import urllib.parse as up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a concatenated tldr string of papers from a certain query. Also return the information pack of these papers.\n",
    "def SSSQuery(query, num_item=50, offset=0, fos=None):\n",
    "    # Send a Semantic API post to get the result paper list\n",
    "    query = '+'.join(query.split())\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/search?query={query}&offset={offset}&limit={num_item}&fields=fieldsOfStudy,abstract'\n",
    "    paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "    paper_list = json.loads(paper_list)\n",
    "\n",
    "    tldr_cat = ''\n",
    "    info_pack = []\n",
    "    # Loop through the paper list and get information pack for each paper\n",
    "    for paper in paper_list['data']:\n",
    "        paper_id = paper['paperId']\n",
    "        # Each paper info pack consists title, abstract, authors, tldr, citation count, field of study\n",
    "        paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=title,abstract,citations.authors,tldr,citationCount,fieldsOfStudy'\n",
    "        paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "        paper_info = json.loads(paper_info)\n",
    "        # Filter out the papers out of field of study\n",
    "        if fos is not None and fos not in paper_info['fieldsOfStudy']:\n",
    "            continue\n",
    "        info_pack.append(paper_info)\n",
    "        tldr = paper_info['tldr']\n",
    "        if tldr is not None:\n",
    "            tldr_cat += ' '+tldr['text'].strip()\n",
    "    tldr_cat = tldr_cat.strip()\n",
    "    return tldr_cat, info_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4100, 2250, 1413, 531, 491, 313, 146, 183, 184, 134]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tldr_cat, info_pack = SSSQuery('transformer', num_item=10, fos='Computer Science')\n",
    "print([i['citationCount'] for i in info_pack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work introduces a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network, and can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps. This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Following prior work on long-sequence transformers, the Longformer is evaluated on character-level language modeling and achieves state-of-the-art results on text8 and enwik8 and pretrain Longformer and finetune it on a variety of downstream tasks. This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences. This work proposes the convolution-augmented transformer for speech recognition, named Conformer, which significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. The proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9–21 on various downstream tasks, and the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. This paper pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language, and showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. LayerDrop, a form of structured dropout, is explored, which has a regularization effect during training and allows for efficient pruning at inference time, and shows that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tldr_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_item = 30\n",
    "offset = 0\n",
    "query = 'hand'\n",
    "query = '+'.join(query.split())\n",
    "url = f'https://api.semanticscholar.org/graph/v1/paper/search?fos[0]=computer-science&query={query}&offset={offset}&limit={num_item}'\n",
    "paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "paper_list = json.loads(paper_list)\n",
    "# print(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in paper_list['data']:\n",
    "    paper_id = paper['paperId']\n",
    "    paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=citations.authors,tldr,citationCount,fieldsOfStudy'\n",
    "    paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "    paper_info = json.loads(paper_info)\n",
    "    print(paper_info['citationCount'], paper_info['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ae9f0abca11b5964ed3b2e1eeca7b20601143dc7930a532e9cd7a2d36507b9d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('carla': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
