{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import yake\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import codecs\n",
    "import urllib.parse as up\n",
    "from crawler import get_paper_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler and Semantic Scholar Information Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a concatenated tldr string of papers from a certain query. Also return the information pack of these papers.\n",
    "def SSSQuery(query, num_item=50, offset=0, fos=None):\n",
    "    # Send a Semantic API post to get the result paper list\n",
    "    query = '+'.join(query.split())\n",
    "    # query_num = int(num_item*1.1)\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/search?query={query}&offset={offset}&limit={num_item}&fields=fieldsOfStudy,abstract'\n",
    "    print(url)\n",
    "    paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "    paper_list = json.loads(paper_list)\n",
    "\n",
    "    tldr_cat = ''\n",
    "    info_pack = []\n",
    "    # Loop through the paper list and get information pack for each paper\n",
    "    for paper in paper_list['data']:\n",
    "        paper_id = paper['paperId']\n",
    "        # Each paper info pack consists title, abstract, authors, tldr, citation count, field of study\n",
    "        paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=title,abstract,citations.authors,tldr,citationCount,influentialCitationCount,fieldsOfStudy'\n",
    "        paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "        paper_info = json.loads(paper_info)\n",
    "        # Filter out the papers out of field of study\n",
    "        if fos is not None and fos not in paper_info['fieldsOfStudy']:\n",
    "            continue\n",
    "        tldr = paper_info['tldr']\n",
    "        if tldr is not None:\n",
    "            tldr_cat += ' '+tldr['text'].strip()\n",
    "            info_pack.append(paper_info)\n",
    "    tldr_cat = tldr_cat.strip()\n",
    "    return tldr_cat, info_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_map(func, data, processes=None):\n",
    "    if processes is None:\n",
    "        processes = mp.cpu_count()\n",
    "    with mp.Pool(processes) as pool:\n",
    "        results = pool.map(func, data)\n",
    "    return results\n",
    "\n",
    "def crawl_one(url):\n",
    "    try:\n",
    "        paper_batch = str(request.urlopen(url).read(), 'utf-8')\n",
    "    except Exception as e:\n",
    "        with open('log.txt', 'a+') as f:\n",
    "            print(f'[ERROR]: {str(e)}')\n",
    "            f.writelines(str(e)+f': {url}\\n')\n",
    "        time.sleep(360)\n",
    "        crawl_one(url)\n",
    "\n",
    "# Generate a concatenated tldr string of papers from a certain query. Also return the information pack of these papers.\n",
    "def SSSQueryMulti(query, num_item=50, offset=0, fos=None):\n",
    "    global info_pack\n",
    "    # Send a Semantic API post to get the result paper list\n",
    "    query = '+'.join(query.split())\n",
    "    get_paper_info_fos = partial(get_paper_info, fos=fos)\n",
    "\n",
    "    # query_num = int(num_item*1.1)\n",
    "    if num_item <= 98:\n",
    "        url = f'https://api.semanticscholar.org/graph/v1/paper/search?query={query}&offset={offset}&limit={num_item}&fields=fieldsOfStudy,abstract'\n",
    "        print(url)\n",
    "        paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "        paper_list = json.loads(paper_list)['data']\n",
    "        info_pack = multi_map(get_paper_info_fos, paper_list)\n",
    "    else:\n",
    "        query_idx = 0\n",
    "        error_count = 0\n",
    "        # paper_list = []\n",
    "        info_pack = []\n",
    "        while query_idx < num_item:\n",
    "            limit = 98 if query_idx + 98 < num_item else num_item - query_idx\n",
    "            url = f'https://api.semanticscholar.org/graph/v1/paper/search?query={query}&offset={query_idx+offset}&limit={limit}&fields=fieldsOfStudy,abstract'\n",
    "            print(url)\n",
    "            try:\n",
    "                paper_batch = str(request.urlopen(url).read(), 'utf-8')\n",
    "            except Exception as e:\n",
    "                with open('log.txt', 'a+') as f:\n",
    "                    print(f'[ERROR]: {str(e)}')\n",
    "                    f.writelines(str(e)+f': {url}\\n')\n",
    "                error_count += 1\n",
    "                if error_count <= 5:\n",
    "                    print(f'ERROR. Start sleeping for 6 min...')\n",
    "                    time.sleep(360)\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            paper_batch = json.loads(paper_batch)['data']\n",
    "            # paper_list += paper_batch\n",
    "            query_idx += 98\n",
    "            if paper_batch is not None:\n",
    "                info_pack += multi_map(get_paper_info_fos, paper_batch)\n",
    "            \n",
    "            # time.sleep(360)\n",
    "\n",
    "    info_pack = [i for i in info_pack if i is not None]\n",
    "    print(f\"Number of papers: {len(info_pack)}\")\n",
    "    # tldr_cat = ' '.join([i['tldr']['text'] for i in info_pack])\n",
    "    return info_pack #tldr_cat, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kws = ['transformer', 'attention']\n",
    "info_pack = []\n",
    "info_pack_all_kws = []\n",
    "kws = pd.read_csv(r'./data/keyword_rest.csv', header=None)\n",
    "kws = [i.lower() for i in kws[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.semanticscholar.org/graph/v1/paper/search?query=ethics+and+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=ethics+and+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=ethics+and+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "[ERROR]: HTTP Error 500: Internal Server Error\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=ethics+and+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "[ERROR]: HTTP Error 500: Internal Server Error\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=ethics+and+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "[ERROR]: HTTP Error 500: Internal Server Error\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=ethics+and+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "[ERROR]: HTTP Error 500: Internal Server Error\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=ethics+and+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "[ERROR]: HTTP Error 500: Internal Server Error\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=ethics+and+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "[ERROR]: HTTP Error 500: Internal Server Error\n",
      "Number of papers: 33\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 272\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=information+retrieval+and+text+mining&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=information+retrieval+and+text+mining&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=information+retrieval+and+text+mining&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 261\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=interpretability+and+analysis+of+models&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=interpretability+and+analysis+of+models&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=interpretability+and+analysis+of+models&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 53\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=linguistic+theories+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=linguistic+theories+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=linguistic+theories+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 180\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=psycholinguistics+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=psycholinguistics+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=psycholinguistics+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 224\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=cognitive+modeling+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=cognitive+modeling+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=cognitive+modeling+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 183\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning+for+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning+for+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning+for+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 258\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=multilinguality+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=multilinguality+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=multilinguality+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 263\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=machine+translation&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=machine+translation&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=machine+translation&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 286\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=nlp+applications&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=nlp+applications&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=nlp+applications&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 264\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=phonology+morphology+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=phonology+morphology+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=phonology+morphology+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 125\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=word+segmentation+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=word+segmentation+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=word+segmentation+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 258\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=question+answering&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=question+answering&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=question+answering&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 285\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=resources+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=resources+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=resources+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 268\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=evaluation+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=evaluation+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=evaluation+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 268\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=sentiment+analysis+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=sentiment+analysis+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=sentiment+analysis+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 260\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=stylistic+analysis+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=stylistic+analysis+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=stylistic+analysis+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 199\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=argument+mining+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=argument+mining+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=argument+mining+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 252\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=semantics+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=semantics+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=semantics+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 267\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=text+summarization+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=text+summarization+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=text+summarization+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 212\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=simple+and+efficient+natural+language+processing&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=simple+and+efficient+natural+language+processing&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=simple+and+efficient+natural+language+processing&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 247\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=syntax+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=syntax+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=syntax+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 253\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=text+summarization+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=text+summarization+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=text+summarization+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 209\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=computational+linguistics+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=computational+linguistics+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=computational+linguistics+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 228\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=transformer+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=transformer+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=transformer+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 241\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=probe+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=probe+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=probe+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 221\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=bert+nlp&offset=0&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=bert+nlp&offset=98&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=bert+nlp&offset=196&limit=98&fields=fieldsOfStudy,abstract\n",
      "Done. Start sleeping for 6 min...\n",
      "Number of papers: 262\n",
      "6332\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "# info_pack_all_kws = []\n",
    "for kw in kws:\n",
    "    info_pack = SSSQueryMulti(kw, num_item=294, fos='Computer Science')\n",
    "    info_pack_all_kws += info_pack\n",
    "print(len(info_pack_all_kws))\n",
    "# except Exception as e:\n",
    "#     with open('log.txt', 'a+') as f:\n",
    "#             f.writelines(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219\n",
      "272\n",
      "272\n"
     ]
    }
   ],
   "source": [
    "print(len( [i['tldr']['text'] for i in info_pack_all_kws]))\n",
    "print(len([np.max((np.log10(i['citationCount']+0.01),0)) for i in info_pack]))\n",
    "print(len([np.max((np.log10(i['influentialCitationCount']+0.01),0)) for i in info_pack]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlib import mio\n",
    "info_top = mio.load('data/transformer_citation_top5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219 7551 6332\n"
     ]
    }
   ],
   "source": [
    "info_backup = info_pack_all_kws\n",
    "\n",
    "info_real_all = info_top + info_pack_all_kws\n",
    "info_real_all_set = list({v['tldr']['text']:v for v in info_real_all}.values())\n",
    "print(len(info_top), len(info_real_all), len(info_backup), len(info_real_all_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>influentialCitationCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The term computational social science of disas...</td>\n",
       "      <td>0.778874</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wearable healthcare data sensors, global posit...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The extent to which big data analytics and dat...</td>\n",
       "      <td>1.230704</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The emergence of a computational social scienc...</td>\n",
       "      <td>0.699838</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is argued against the assertion that theory...</td>\n",
       "      <td>2.421620</td>\n",
       "      <td>0.845718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  citationCount  \\\n",
       "0  The term computational social science of disas...       0.778874   \n",
       "1  Wearable healthcare data sensors, global posit...       0.000000   \n",
       "2  The extent to which big data analytics and dat...       1.230704   \n",
       "3  The emergence of a computational social scienc...       0.699838   \n",
       "4  It is argued against the assertion that theory...       2.421620   \n",
       "\n",
       "   influentialCitationCount  \n",
       "0                  0.000000  \n",
       "1                  0.000000  \n",
       "2                  0.000000  \n",
       "3                  0.000000  \n",
       "4                  0.845718  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = 'transformer_citation_all_set'\n",
    "ip = info_real_all_set\n",
    "df = pd.DataFrame({'text': [i['tldr']['text'] for i in ip], 'citationCount': [np.max((np.log10(i['citationCount']+0.01),0)) for i in ip], 'influentialCitationCount':[np.max((np.log10(i['influentialCitationCount']+0.01),0)) for i in ip]})\n",
    "df.to_csv(f'data/{stem}.csv', index=None)\n",
    "with open(f'data/{stem}.pkl', 'wb') as f:\n",
    "    pkl.dump(ip, f)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4951\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>influentialCitationCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The term computational social science of disas...</td>\n",
       "      <td>0.778874</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The extent to which big data analytics and dat...</td>\n",
       "      <td>1.230704</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The emergence of a computational social scienc...</td>\n",
       "      <td>0.699838</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is argued against the assertion that theory...</td>\n",
       "      <td>2.421620</td>\n",
       "      <td>0.845718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This paper argues for a comprehensive and bala...</td>\n",
       "      <td>0.778874</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  citationCount  \\\n",
       "0  The term computational social science of disas...       0.778874   \n",
       "1  The extent to which big data analytics and dat...       1.230704   \n",
       "2  The emergence of a computational social scienc...       0.699838   \n",
       "3  It is argued against the assertion that theory...       2.421620   \n",
       "4  This paper argues for a comprehensive and bala...       0.778874   \n",
       "\n",
       "   influentialCitationCount  \n",
       "0                  0.000000  \n",
       "1                  0.000000  \n",
       "2                  0.000000  \n",
       "3                  0.845718  \n",
       "4                  0.000000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = 'transformer_citation_filtered_set'\n",
    "info_pack_valid = [i for i in info_real_all_set if i['citationCount']>1]\n",
    "print(len(info_pack_valid))\n",
    "ip = info_pack_valid\n",
    "df = pd.DataFrame({'text': [i['tldr']['text'] for i in ip], 'citationCount': [np.max((np.log10(i['citationCount']+0.01),0)) for i in ip], 'influentialCitationCount':[np.max((np.log10(i['influentialCitationCount']+0.01),0)) for i in ip]})\n",
    "df.to_csv(f'data/{stem}.csv', index=None)\n",
    "with open(f'data/{stem}.pkl', 'wb') as f:\n",
    "    pkl.dump(ip, f)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid = df[df.citationCount.ge(0.1)]\n",
    "# with open(f'data/{stem}.pkl', 'wb') as f:\n",
    "#     pkl.dump(info_pack_all_kws, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.semanticscholar.org/graph/v1/paper/search?query=transformer&offset=0&limit=100&fields=fieldsOfStudy,abstract\n",
      "[4114, 2267, 1421, 533, 493, 320, 148, 185, 187, 138, 124, 127, 116, 356, 240, 172, 102, 80, 68, 231, 185, 167, 161, 394, 167, 282, 299, 176, 144, 52, 219, 124, 143, 182, 102, 107, 446, 92, 287, 134, 77, 67, 61, 58, 57, 55, 62, 44, 78, 80, 108, 130, 92, 62, 168, 153, 95, 92, 45, 88, 39, 77, 62, 60, 141, 111, 93, 256, 177]\n",
      "[527, 416, 183, 131, 72, 63, 39, 50, 31, 27, 9, 16, 9, 70, 42, 43, 6, 24, 9, 30, 29, 31, 29, 124, 27, 25, 20, 7, 29, 2, 42, 33, 21, 53, 7, 14, 26, 7, 26, 43, 11, 3, 8, 11, 3, 3, 1, 7, 25, 14, 15, 10, 15, 6, 10, 8, 8, 8, 8, 13, 5, 16, 1, 1, 7, 18, 5, 16, 4]\n"
     ]
    }
   ],
   "source": [
    "tldr_cat, info_pack = SSSQuery('transformer', num_item=100, fos='Computer Science')\n",
    "print([i['citationCount'] for i in info_pack])\n",
    "print([i['influentialCitationCount'] for i in info_pack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work introduces a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network, and can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_pack[0]['tldr']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miracleyoo\\Anaconda3\\envs\\cs685\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log10\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max((np.log10(0), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>influentialCitationCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A new simple network architecture, the Transfo...</td>\n",
       "      <td>4.417571</td>\n",
       "      <td>3.773421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A global approach which always attends to all ...</td>\n",
       "      <td>3.734320</td>\n",
       "      <td>2.742733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An attention based model that automatically le...</td>\n",
       "      <td>3.826464</td>\n",
       "      <td>2.769385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The proposed SAGAN achieves the state-of-the-a...</td>\n",
       "      <td>3.234520</td>\n",
       "      <td>2.287824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New state-of-the-art segmentation performance ...</td>\n",
       "      <td>3.157762</td>\n",
       "      <td>2.206853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  citationCount  \\\n",
       "0  A new simple network architecture, the Transfo...       4.417571   \n",
       "1  A global approach which always attends to all ...       3.734320   \n",
       "2  An attention based model that automatically le...       3.826464   \n",
       "3  The proposed SAGAN achieves the state-of-the-a...       3.234520   \n",
       "4  New state-of-the-art segmentation performance ...       3.157762   \n",
       "\n",
       "   influentialCitationCount  \n",
       "0                  3.773421  \n",
       "1                  2.742733  \n",
       "2                  2.769385  \n",
       "3                  2.287824  \n",
       "4                  2.206853  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': [i['tldr']['text'] for i in info_pack], 'citationCount': [np.max((np.log10(i['citationCount']+0.01),0)) for i in info_pack], 'influentialCitationCount':[np.max((np.log10(i['influentialCitationCount']+0.01),0)) for i in info_pack]})\n",
    "df.to_csv('transformer_citation.csv', index=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work introduces a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network, and can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps. This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Following prior work on long-sequence transformers, the Longformer is evaluated on character-level language modeling and achieves state-of-the-art results on text8 and enwik8 and pretrain Longformer and finetune it on a variety of downstream tasks. This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences. This work proposes the convolution-augmented transformer for speech recognition, named Conformer, which significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. The proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9–21 on various downstream tasks, and the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. This paper pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language, and showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. LayerDrop, a form of structured dropout, is explored, which has a regularization effect during training and allows for efficient pruning at inference time, and shows that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tldr_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Spatial', 0.03213113441634914)\n",
      "('work', 0.04798022059872212)\n",
      "('language', 0.06973861700167353)\n",
      "('Transformer', 0.07994217878795795)\n",
      "('tasks', 0.08425923835568348)\n",
      "('architectures', 0.12034394262796394)\n",
      "('Longformer', 0.1231255301810972)\n",
      "('neural', 0.13795581464104506)\n",
      "('training', 0.13840506098154778)\n",
      "('module', 0.14654904914113015)\n",
      "('giving', 0.14654904914113015)\n",
      "('maps', 0.14654904914113015)\n",
      "('achieves', 0.1514314025909243)\n",
      "('results', 0.1514314025909243)\n",
      "('introduces', 0.15751851687783422)\n",
      "('learnable', 0.15751851687783422)\n",
      "('explicitly', 0.15751851687783422)\n",
      "('manipulation', 0.15751851687783422)\n",
      "('data', 0.15751851687783422)\n",
      "('inserted', 0.15751851687783422)\n"
     ]
    }
   ],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 20\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(tldr_cat)\n",
    "keywords = sorted(keywords, key=lambda x:x[1])\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neural architecture transformer', 0.5527), ('length disrupting', 0.1076), ('benchmarks covering summarization', 0.2611), ('language showed newly', 0.2474), ('explicitly allows spatial', 0.2433), ('replaces dot product', 0.2065), ('answering', 0.2506), ('finetune variety downstream', 0.0994), ('sampling algorithm hgsampling', -0.0439), ('nlp tasks layerdrop', 0.323)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# doc = \"\"\"\n",
    "#          Supervised learning is the machine learning task of learning a function that\n",
    "#          maps an input to an output based on example input-output pairs. It infers a\n",
    "#          function from labeled training data consisting of a set of training examples.\n",
    "#          In supervised learning, each example is a pair consisting of an input object\n",
    "#          (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "#          A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "#          which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "#          algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "#          the learning algorithm to generalize from the training data to unseen situations in a \n",
    "#          'reasonable' way (see inductive bias).\n",
    "#       \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(tldr_cat, keyphrase_ngram_range=(1,3), stop_words='english', top_n = 10, nr_candidates=40, use_mmr=True, diversity=0.7)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_item = 30\n",
    "offset = 0\n",
    "query = 'hand'\n",
    "query = '+'.join(query.split())\n",
    "url = f'https://api.semanticscholar.org/graph/v1/paper/search?fos[0]=computer-science&query={query}&offset={offset}&limit={num_item}'\n",
    "paper_list = str(request.urlopen(url).read(), 'utf-8')\n",
    "paper_list = json.loads(paper_list)\n",
    "# print(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in paper_list['data']:\n",
    "    paper_id = paper['paperId']\n",
    "    paper_url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=citations.authors,tldr,citationCount,fieldsOfStudy'\n",
    "    paper_info = str(request.urlopen(paper_url).read(), 'utf-8')\n",
    "    paper_info = json.loads(paper_info)\n",
    "    print(paper_info['citationCount'], paper_info['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ae9f0abca11b5964ed3b2e1eeca7b20601143dc7930a532e9cd7a2d36507b9d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('carla': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
